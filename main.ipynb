{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-05T08:14:32.394674Z",
     "start_time": "2025-12-05T08:14:30.269024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "from aie_project.training import EfficientClassificationConfig, EfficientClassificationModel\n",
    "from aie_project.training.dataset_utils import easy_load"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T08:14:34.694294Z",
     "start_time": "2025-12-05T08:14:33.064212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset, label2id, id2label = easy_load(\"./datasets/recyclables_image_classification\")\n",
    "\n",
    "train_ds = dataset[\"train\"]\n",
    "val_ds = dataset[\"validation\"]"
   ],
   "id": "24f7de1447e6e8fb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/42 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "815674fd8d8d41309aa5e03e464e22cb"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_ds[0][\"image\"]",
   "id": "3bdeeac0f53f45b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_ds[0][\"pixel_values\"]",
   "id": "2475829868d63b3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T08:14:37.934544Z",
     "start_time": "2025-12-05T08:14:37.196366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "config = EfficientClassificationConfig(\n",
    "    num_labels=len(label2id),\n",
    "    backbone_name=\"mobilenetv3_large_100\",\n",
    "    pretrained=True,\n",
    "    img_size=224,\n",
    "    classifier_dropout=0.2,\n",
    ")\n",
    "\n",
    "model = EfficientClassificationModel(config)"
   ],
   "id": "c47f76e79ed4814b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# collator = DefaultDataCollator()\n",
    "# raw_items = [{\n",
    "#     \"pixel_values\": train_ds[0][\"pixel_values\"],\n",
    "#     \"labels\": train_ds[0][\"labels\"],\n",
    "# }]\n",
    "# batch = collator(raw_items)\n",
    "# outputs = model(**batch)"
   ],
   "id": "ed088d87469485a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T08:14:39.478289Z",
     "start_time": "2025-12-05T08:14:39.370115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# run training\n",
    "from transformers import TrainingArguments, Trainer"
   ],
   "id": "5fd2e9db44a87a93",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T08:14:40.155066Z",
     "start_time": "2025-12-05T08:14:40.065824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=None,\n",
    ")"
   ],
   "id": "18b0aef1c30d4b13",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/py/1x1r0phj3wx__zpnnn9q9wtr0000gn/T/ipykernel_83914/2765267914.py:14: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T08:14:58.154899Z",
     "start_time": "2025-12-05T08:14:41.239569Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "f3a8e79143864779",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/riverfog7/Workspace/ai_experiment_project/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24' max='24168' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   24/24168 00:13 < 4:07:28, 1.63 it/s, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "68fa9d1571c7cd1719e53a8a82cae3a1"
     }
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Workspace/ai_experiment_project/.venv/lib/python3.12/site-packages/transformers/trainer.py:2325\u001B[39m, in \u001B[36mTrainer.train\u001B[39m\u001B[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[39m\n\u001B[32m   2323\u001B[39m         hf_hub_utils.enable_progress_bars()\n\u001B[32m   2324\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2325\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2326\u001B[39m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[43m=\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2327\u001B[39m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m=\u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2328\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2329\u001B[39m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m=\u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2330\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Workspace/ai_experiment_project/.venv/lib/python3.12/site-packages/transformers/trainer.py:2679\u001B[39m, in \u001B[36mTrainer._inner_training_loop\u001B[39m\u001B[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[39m\n\u001B[32m   2673\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[32m   2674\u001B[39m     tr_loss_step = \u001B[38;5;28mself\u001B[39m.training_step(model, inputs, num_items_in_batch)\n\u001B[32m   2676\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m   2677\u001B[39m     args.logging_nan_inf_filter\n\u001B[32m   2678\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[32m-> \u001B[39m\u001B[32m2679\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m (torch.isnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43misinf\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtr_loss_step\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m   2680\u001B[39m ):\n\u001B[32m   2681\u001B[39m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[32m   2682\u001B[39m     tr_loss = tr_loss + tr_loss / (\u001B[32m1\u001B[39m + \u001B[38;5;28mself\u001B[39m.state.global_step - \u001B[38;5;28mself\u001B[39m._globalstep_last_logged)\n\u001B[32m   2683\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "2f51ba66904c6e2b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
